{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ale_py\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLjVOZIcqkBz",
        "outputId": "ae50c284-bf13-4e98-9f74-244d0600125c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale_py in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale_py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale_py) (4.12.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbmNLlIqqOLz"
      },
      "outputs": [],
      "source": [
        "import ale_py, cv2, os, random, torch\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, inputShape, numActions):\n",
        "        super(DQNetwork, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(inputShape[0], 32, kernel_size=8, stride=4),  # Convolutional layers\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.featureSize(inputShape), 512),  # Fully connected layers\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, numActions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        return self.fc(x)\n",
        "\n",
        "    # Gets size of features for FC layer\n",
        "    def featureSize(self, inputShape):\n",
        "        return self.conv(torch.zeros(1, *inputShape)).view(1, -1).size(1)"
      ],
      "metadata": {
        "id": "gaMrEqfxqV_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, stateShape, numActions, gamma = 0.95, lr=0.01, epsilonMax =1.0, epsilonDecay=0.999, epsilonMin=0.0):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        self.numActions = numActions\n",
        "\n",
        "        # Networks\n",
        "        self.qNetwork = DQNetwork(stateShape, numActions).to(self.device)\n",
        "        self.targetNetwork = DQNetwork(stateShape, numActions).to(self.device)  # Stabilizes training by keeping the target Q-values fixed for several updates.\n",
        "        self.targetNetwork.load_state_dict(self.qNetwork.state_dict())\n",
        "        self.targetNetwork.eval()\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilonMax\n",
        "        self.epsilonDecay = epsilonMax / 50000\n",
        "        self.epsilonMin = epsilonMin\n",
        "        self.lr = lr\n",
        "        self.batchSize = 2048\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.optimizer = optim.Adam(self.qNetwork.parameters(), lr=self.lr)\n",
        "        self.errors = []\n",
        "\n",
        "    def action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.numActions - 1)  # Explore\n",
        "        else:\n",
        "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "            qValues = self.qNetwork(state)\n",
        "            return torch.argmax(qValues).item()  # Exploit\n",
        "\n",
        "    # Captures agent experience and stores it into the replay buffer\n",
        "    def storeExperience(self, state, action, reward, nextState, done):\n",
        "        self.memory.append((state, action, reward, nextState, done))\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.memory) < self.batchSize:\n",
        "            return\n",
        "\n",
        "        # Sample mini-batch from memory\n",
        "        batch = random.sample(self.memory, self.batchSize)\n",
        "        states, actions, rewards, nextStates, dones = zip(*batch)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        nextStates = np.array(nextStates)\n",
        "        dones = np.array(dones)\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        nextStates = torch.tensor(nextStates, dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        # Compute target Q-values\n",
        "        with torch.no_grad():\n",
        "            maxNextQValues = self.targetNetwork(nextStates).max(1)[0]\n",
        "            targetQValues = rewards + self.gamma * maxNextQValues * (1 - dones)\n",
        "\n",
        "        # Compute current Q-values\n",
        "        qValues = self.qNetwork(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Compute and log TD error\n",
        "        self.errors.append(torch.abs(targetQValues - qValues).detach().cpu().numpy())\n",
        "\n",
        "        # Compute loss and optimize\n",
        "        loss = nn.MSELoss()(qValues, targetQValues)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon\n",
        "        if done:\n",
        "            self.epsilon = max(self.epsilonMin, self.epsilon - self.epsilonDecay)\n",
        "\n",
        "    def updateTargetNetwork(self):\n",
        "        self.targetNetwork.load_state_dict(self.qNetwork.state_dict())"
      ],
      "metadata": {
        "id": "kZIOMaYsqYaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessState(state):\n",
        "    import cv2\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
        "    state = cv2.resize(state, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    state = state / 255.0\n",
        "    return np.expand_dims(state, axis=0)  # Add channel dimension"
      ],
      "metadata": {
        "id": "yioCZ_Vdqazm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/Frogger-v5\", render_mode=\"rgb_array\")\n",
        "stateShape = (1, 84, 84)  # Grayscale, resized to 84x84\n",
        "numActions = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(stateShape, numActions)\n",
        "episodes = 50000\n",
        "targetUpdateFreq = 500  # How frequently the target network is updated\n",
        "recordingPeriod = 10000\n",
        "episodeRewards = []\n",
        "trainingErrors = []\n",
        "\n",
        "env = RecordVideo(env, video_folder=\"frogger-deep-agent\", name_prefix=f\"eval_{episodes}\",\n",
        "                  episode_trigger=lambda x: x % recordingPeriod == 0)\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = preprocessState(state)\n",
        "    totalReward = 0\n",
        "    done = False\n",
        "    agent.errors = []\n",
        "\n",
        "    while not done:\n",
        "        action = agent.action(state)\n",
        "        nextState, reward, done, truncated, _ = env.step(action)\n",
        "        nextState = preprocessState(nextState)\n",
        "\n",
        "        # Store experience and update agent\n",
        "        agent.storeExperience(state, action, reward, nextState, done)\n",
        "\n",
        "        totalReward += reward\n",
        "        state = nextState\n",
        "\n",
        "    agent.update()\n",
        "\n",
        "    # Update target network\n",
        "    if episode % targetUpdateFreq == 0:\n",
        "        agent.updateTargetNetwork()\n",
        "\n",
        "    episodeRewards.append(totalReward)  # Total Rewards\n",
        "\n",
        "    if len(trainingErrors) > 0:\n",
        "      trainingErrors.append(np.mean(agent.errors))  # Training Error\n",
        "    else:\n",
        "      print(f\"Warning: training_errors is empty in episode {episode}\")\n",
        "      trainingErrors.append(0)\n",
        "\n",
        "    if episode % 2500 == 0:\n",
        "        print(f\"Episode {episode}/{episodes}, Total Reward: {totalReward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-Ox5Twyqc1c",
        "outputId": "f0e58347-25c3-4475-cd5b-ddd0d5a67e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/frogger-deep-agent folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: training_errors is empty in episode 0\n",
            "Episode 0/50000, Total Reward: 15.0, Epsilon: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 2500/50000, Total Reward: 10.0, Epsilon: 0.95\n",
            "Episode 5000/50000, Total Reward: 9.0, Epsilon: 0.90\n",
            "Episode 7500/50000, Total Reward: 8.0, Epsilon: 0.85\n",
            "Episode 10000/50000, Total Reward: 9.0, Epsilon: 0.80\n",
            "Episode 12500/50000, Total Reward: 8.0, Epsilon: 0.75\n",
            "Episode 15000/50000, Total Reward: 11.0, Epsilon: 0.70\n",
            "Episode 17500/50000, Total Reward: 23.0, Epsilon: 0.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Compute rolling averages\n",
        "rollingWindow = 2500\n",
        "rewards = np.convolve(episodeRewards, np.ones(rollingWindow), mode='valid')\n",
        "errors = np.convolve(trainingErrors, np.ones(rollingWindow), mode='valid')\n",
        "\n",
        "# Episode Rewards\n",
        "axs[0].plot(rewards)\n",
        "axs[0].set_title(\"Episode Rewards\")\n",
        "axs[0].set_xlabel(\"Episode\")\n",
        "axs[0].set_ylabel(\"Reward\")\n",
        "\n",
        "# Training Errors\n",
        "axs[1].plot(errors)\n",
        "axs[1].set_title(\"Training Error\")\n",
        "axs[1].set_xlabel(\"Episode\")\n",
        "axs[1].set_ylabel(\"Temporal Difference Error\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "zHR2PGKlqe_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}